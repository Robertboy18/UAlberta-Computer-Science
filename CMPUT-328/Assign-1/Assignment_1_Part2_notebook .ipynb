{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_1_Part2_notebook.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hfn-Yiy1lp5l",
        "colab_type": "text"
      },
      "source": [
        "# Lab Assignment 1 - Part2 \n",
        "## With this assignment you will get to know more about gradient descent optimization and writing your own functions with forward and backward (i.e., gradient) passes\n",
        "## You need to complete all the tasks in this notebook in the lab and show you work to the TA. Edit only those portions in the cells where it asks you to do so!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zp3BetP-d6cB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torch.autograd import Function\n",
        "import torch.nn.functional as F\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usEoWHBnSJW9",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJpovSL8d_-l",
        "colab_type": "text"
      },
      "source": [
        "## Huber loss function\n",
        "https://en.wikipedia.org/wiki/Huber_loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTp4nNf9d-zg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A loss function measures distance between a predicted and a target tensor\n",
        "# An implementation of Huber loss function is given below\n",
        "# We will make use of this loss function in gradient descent optimization\n",
        "def Huber_Loss(input,delta):\n",
        "  m = (torch.abs(input)<=delta).detach().float()\n",
        "  #print(m)  \n",
        "  output = torch.sum(0.5*m*input**2 + delta*(1.0-m)*(torch.abs(input)-0.5*delta))\n",
        "  return output"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZoxXPadgk-O",
        "colab_type": "text"
      },
      "source": [
        "# Test Huber loss with a couple of different examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYO_KmUQfmnm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "135d8336-23d1-4cbe-886f-d687e1bd4233"
      },
      "source": [
        "a = torch.tensor([[0.3, 2.0, -3.1],[0.5, 9.2, 0.1]])\n",
        "print(a.numpy())\n",
        "ha = Huber_Loss(a,1.0)\n",
        "print(ha.numpy())\n",
        "\n",
        "b = torch.tensor([0.3, 2.0])\n",
        "print(b.numpy())\n",
        "hb = Huber_Loss(b,1.0)\n",
        "print(hb.numpy())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.3  2.  -3.1]\n",
            " [ 0.5  9.2  0.1]]\n",
            "12.974999\n",
            "[0.3 2. ]\n",
            "1.545\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26wLACTj7FkG",
        "colab_type": "text"
      },
      "source": [
        "# Gradient descent code\n",
        "## Study the following generic gradient descent optimization code.\n",
        "## Huber loss f measures the distance between a probability vector `z` and target 1-hot vector `target`.\n",
        "## When `f.backward` is called, PyTorch first computes $\\nabla_z f$ (gradient of `f` with respect to `z`), then by chain rule it computes $\\nabla_{var} f = J^{z}_{var} \\nabla_z f$, where $J^{z}_{var}$ is the Jacobian of `z` with respect to `var`.\n",
        "## Next, `optimizer.step()` call adjusts the variable `var` in the opposite direction of $\\nabla_{var} f.$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLxQgQaD7Krq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gradient_descent(var,optimizer,softmax,loss,target,nIter,nPrint):\n",
        "  for i in range(nIter):\n",
        "    z = softmax(var)\n",
        "    f = loss(z-target,1.0)\n",
        "    optimizer.zero_grad()\n",
        "    f.backward()\n",
        "    optimizer.step()\n",
        "    if i%nPrint==0:\n",
        "      with np.printoptions(precision=3, suppress=True):\n",
        "        print(\"Iteration:\",i,\"Variable:\", z.detach().numpy(),\"Loss: %0.6f\" % f.item())\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5viWaJpSiDuN",
        "colab_type": "text"
      },
      "source": [
        "# Gradient descent with Huber Loss\n",
        "## The following cell shows how `gradient_descent` function can be used.\n",
        "## The cell first creates a target 1-hot vector `y`, where only the 3rd place is on.\n",
        "## It also creates a variable `x` with random initialization and an optimizer.\n",
        "## Learning rate and momentum has been set to 0.1 and 0.9, respectively.\n",
        "## Then it calls `gradient_descent` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzRgWv_NiIeQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "332f757e-b6ed-409d-abe0-da3aec20cd34"
      },
      "source": [
        "y = torch.zeros(10)\n",
        "y[2] = 1.0\n",
        "print(\"Target 1-hot vector:\",y.numpy())\n",
        "x = Variable(torch.randn(y.shape),requires_grad=True)\n",
        "\n",
        "optimizer = torch.optim.SGD([x], lr=1e-1, momentum=0.9) # create an optimizer that will do gradient descent optimization\n",
        "\n",
        "gradient_descent(x,optimizer,F.softmax,Huber_Loss,y,1000,100)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Target 1-hot vector: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Iteration: 0 Variable: [0.087 0.011 0.195 0.233 0.016 0.024 0.239 0.15  0.014 0.03 ] Loss: 0.396109\n",
            "Iteration: 100 Variable: [0.008 0.001 0.952 0.01  0.002 0.003 0.01  0.01  0.002 0.003] Loss: 0.001329\n",
            "Iteration: 200 Variable: [0.006 0.001 0.962 0.008 0.002 0.002 0.008 0.007 0.001 0.003] Loss: 0.000832\n",
            "Iteration: 300 Variable: [0.005 0.001 0.968 0.006 0.001 0.002 0.006 0.006 0.001 0.002] Loss: 0.000606\n",
            "Iteration: 400 Variable: [0.005 0.001 0.971 0.006 0.001 0.002 0.006 0.006 0.001 0.002] Loss: 0.000477\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: 500 Variable: [0.004 0.001 0.974 0.005 0.001 0.002 0.005 0.005 0.001 0.002] Loss: 0.000393\n",
            "Iteration: 600 Variable: [0.004 0.001 0.976 0.005 0.001 0.002 0.005 0.005 0.001 0.002] Loss: 0.000334\n",
            "Iteration: 700 Variable: [0.004 0.001 0.978 0.004 0.001 0.001 0.004 0.004 0.001 0.002] Loss: 0.000290\n",
            "Iteration: 800 Variable: [0.003 0.001 0.979 0.004 0.001 0.001 0.004 0.004 0.001 0.002] Loss: 0.000257\n",
            "Iteration: 900 Variable: [0.003 0.001 0.98  0.004 0.001 0.001 0.004 0.004 0.001 0.002] Loss: 0.000230\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtIf2LRqvOph",
        "colab_type": "text"
      },
      "source": [
        "# <font color='red'>30% Weight:</font> In this markdown cell, using [math mode](https://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Working%20With%20Markdown%20Cells.html), write gradient of Huber loss function: $output = \\sum_i 0.5 m_i (input)^{2}_{i} + \\delta (1-m_i)(|input_i|-0.5 \\delta)$ with respect to $input.$ Treat $m_i$ to be independent of $input_i,$ because we replaced *if* control statement with $m_i.$\n",
        "## Your solution <font color='red'>20% (complete formula)</font>: $\\frac{\\partial (output)}{\\partial (input)_i} = ...$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgqXvMYDsUBl",
        "colab_type": "text"
      },
      "source": [
        "**Partial Differentiation**\n",
        "\n",
        "Let $y^{\\hat{}}$ = $output$\n",
        "\n",
        "Let $y_{i}$ = $input_{i}$\n",
        "\n",
        "$y^{\\hat{}} = \\sum_{i = 1}^{n} \\frac{1}{2} m_{i} (y_{i})^2 + \\delta (1-m_{i})(|y_{i}| - \\frac{1}{2}\\delta )$\n",
        "\n",
        "$\\frac{\\partial y^{\\hat{}} }{\\partial y_{i}} = m_{i}y_{i} + \\delta (1-m_{i})(\\frac{y_{i}}{|y_{i}|})$\n",
        "\n",
        "A much better way is to use the sgn function as whenever $y_{i} \\neq 0$ we have \n",
        "\n",
        "${sgn}(x)={x \\over |x|}={|x| \\over x}$\n",
        "Therefore\n",
        "$\\frac{\\partial y^{\\hat{}} }{\\partial y_{i}} = m_{i}y_{i} + \\delta (1-m_{i})({sgn}(y_{i}))$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ly8SaBQ-lXbg",
        "colab_type": "text"
      },
      "source": [
        "# <font color='red'>30% Weight:</font> Define your own (correct!) rule of differentiation for Huber loss function\n",
        "## Edit indicated line in the cell below. Use the following formula. Do not use for/while/any loop in your solution.\n",
        "## For this function,  chain rule (Jacobian-vector product) takes the following form: $\\frac{\\partial (cost)}{\\partial (input)_i} = \\frac{\\partial (output)}{\\partial (input)_i} \\frac{\\partial (cost)}{\\partial (output)}.$\n",
        "# In the `backward` method below, $\\frac{\\partial (cost)}{\\partial (output)}$ is denoted by `output_grad` and the $i^{th}$ component of `input_grad` is symbolized by $\\frac{\\partial (cost)}{\\partial (input)_i}.$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UX4zC76XlWr0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Inherit from torch.autograd.Function\n",
        "class My_Huber_Loss(Function):\n",
        "\n",
        "    # Note that both forward and backward are @staticmethods\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, delta):\n",
        "        m = (torch.abs(input)<=delta).float()\n",
        "        ctx.save_for_backward(input,torch.tensor(m),torch.tensor(delta))\n",
        "        output = torch.sum(0.5*m*input**2 + delta*(1.0-m)*(torch.abs(input)-0.5*delta))\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, output_grad):\n",
        "        # retrieve saved tensors and use them in derivative calculation\n",
        "        input, m, delta = ctx.saved_tensors\n",
        "\n",
        "        \"\"\"\n",
        "        Return Jacobian-vector product (chain rule)\n",
        "        For Huber loss function the Jacobian happens to be a diagonal matrix\n",
        "        Also, note that output_grad is a scalar, because forward function returns a scalar value\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"\n",
        "        As obtained in the above cell by differentiation and using the retrieved saved tensors\n",
        "        Multiply this result by the output_grad to gain back the input_grad by the above formula(chain rule) \n",
        "        \"\"\"\n",
        "        input_grad = (m*input + delta*(1-m)*torch.sign(input))*output_grad \n",
        "        \"\"\"\n",
        "        must return two gradients becuase forward function takes in two arguments\n",
        "        \"\"\"\n",
        "        return input_grad, None"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkG5zXGZcgja",
        "colab_type": "text"
      },
      "source": [
        "#Gradient Descent on Your Own Huber Loss\n",
        "## You should get almost identical results as before if your rule of differentation is correct!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DKnFDK0pPjF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "3d2e7681-832d-4d16-b277-52bf5a62d450"
      },
      "source": [
        "y = torch.zeros(10)\n",
        "y[2] = 1.0\n",
        "print(\"Target:\",y.numpy())\n",
        "x = Variable(torch.randn(y.shape),requires_grad=True)\n",
        "\n",
        "optimizer = torch.optim.SGD([x], lr=1e-1, momentum=0.9) # create an optimizer that will do gradient descent optimization\n",
        "\n",
        "gradient_descent(x,optimizer,F.softmax,My_Huber_Loss.apply,y,1000,100)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Target: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Iteration: 0 Variable: [0.047 0.036 0.037 0.036 0.094 0.142 0.279 0.123 0.126 0.081] Loss: 0.538446\n",
            "Iteration: 100 Variable: [0.005 0.004 0.941 0.004 0.007 0.008 0.008 0.008 0.008 0.007] Loss: 0.001919\n",
            "Iteration: 200 Variable: [0.004 0.003 0.957 0.003 0.005 0.006 0.006 0.006 0.006 0.005] Loss: 0.001035\n",
            "Iteration: 300 Variable: [0.003 0.003 0.964 0.003 0.004 0.005 0.005 0.005 0.005 0.004] Loss: 0.000717\n",
            "Iteration: 400 Variable: [0.003 0.002 0.969 0.002 0.004 0.004 0.004 0.004 0.004 0.004] Loss: 0.000549\n",
            "Iteration: 500 Variable: [0.002 0.002 0.972 0.002 0.004 0.004 0.004 0.004 0.004 0.003] Loss: 0.000444\n",
            "Iteration: 600 Variable: [0.002 0.002 0.974 0.002 0.003 0.003 0.003 0.003 0.003 0.003] Loss: 0.000373\n",
            "Iteration: 700 Variable: [0.002 0.002 0.976 0.002 0.003 0.003 0.003 0.003 0.003 0.003] Loss: 0.000321\n",
            "Iteration: 800 Variable: [0.002 0.002 0.978 0.002 0.003 0.003 0.003 0.003 0.003 0.003] Loss: 0.000282\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: 900 Variable: [0.002 0.002 0.979 0.002 0.003 0.003 0.003 0.003 0.003 0.003] Loss: 0.000251\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2K4Q7ePPdfy",
        "colab_type": "text"
      },
      "source": [
        "# <font color='red'>40% Weight:</font> Your own softmax with forward and backward functions\n",
        "## Edit indicated line in the cell below. Use the following formula. Do not use for/while/any loop in your solution.\n",
        "## The Jacobian-vector product (chain rule) takes the following form using summation sign: $\\frac{\\partial (cost)}{\\partial (input)_i} = \\sum_j \\frac{\\partial (output)_j}{\\partial (input)_i} \\frac{\\partial (cost)}{\\partial (output)_j}$\n",
        "# Once again note that, in the `backward` method below, $i^{th}$ component of `input_grad` and $j^{th}$ component of `output_grad` are denoted by $\\frac{\\partial (cost)}{\\partial (input)_i}$ and $\\frac{\\partial (cost)}{\\partial (output)_j}$, respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zn52-xK_PijV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Inherit from Function\n",
        "class My_softmax(Function):\n",
        "\n",
        "    # Note that both forward and backward are @staticmethods\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        output = F.softmax(input,dim=0)\n",
        "        ctx.save_for_backward(output) # this is the only tensor you will need to save for backward function\n",
        "        return output\n",
        "\n",
        "    # This function has only a single output, so it gets only one gradient\n",
        "    @staticmethod\n",
        "    def backward(ctx, output_grad):\n",
        "        output = ctx.saved_tensors[0]\n",
        "        #print(output.dtype)\n",
        "        # retrieve saved tensors and use them in derivative calculation\n",
        "        # return Jacobian-vecor product\n",
        "\n",
        "        matrix = np.outer(output,output)\n",
        "        output_product = torch.tensor(matrix,dtype = torch.float32)\n",
        "        \"\"\"\n",
        "        torch.diag = gives the diagonal of the tensor, offset = 0 returns the main diagonal \n",
        "        torch.ger = gives the outer product of two matrices\n",
        "        torch.sum = returns the sum of the input elements in the tensor along the specified dimension\n",
        "        dimnesion = 1 as returning the sum along the column\n",
        "        \"\"\"\n",
        "        \n",
        "\n",
        "        \"\"\"\n",
        "        Numpy outer product is much faster than torch.ger \n",
        "        When checking the results - numpy outer product however yields a bigger difference from the actual values\n",
        "        reference = https://stackoverflow.com/questions/54357836/numpy-is-faster-than-pytorch-for-larger-cross-or-outer-products\n",
        "        \"\"\"\n",
        "        # input_grad = torch.sum((torch.diag(output, 0) - output_product) *output_grad, dim = 1)\n",
        "\n",
        "        \"\"\"\n",
        "        Now the reason why we are taking the diagonal matrix is because when i =\\= j the partial derivative of the softmax function\n",
        "        i =\\= j : '0' - outer product of the product n * n matrix as the main diagonal contains all 0 entries\n",
        "        i = j : diagnonal entries of the matrix - the outer product \n",
        "        This in turn gets multiplied by the output_grad by the chain rule\n",
        "        Along each column the sum is taken with respect to each i thus dim = 1\n",
        "        \"\"\"\n",
        "        input_grad = torch.sum((torch.diag(output, 0) - torch.ger(output, output)) *output_grad, dim = 1) \n",
        "        return input_grad"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcixVFs4cwHO",
        "colab_type": "text"
      },
      "source": [
        "# Gradient Descent on your own Huber Loss and your own softmax"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UejqQeb4RZk0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "dc21400d-4643-4df8-8e58-82e12547416b"
      },
      "source": [
        "y = torch.zeros(10)\n",
        "y[2] = 1.0\n",
        "print(y)\n",
        "x = Variable(torch.randn(y.shape),requires_grad=True)\n",
        "print(x)\n",
        "\n",
        "optimizer = torch.optim.SGD([x], lr=1e-1, momentum=0.9) # create an optimizer that will do gradient descent optimization\n",
        "\n",
        "gradient_descent(x,optimizer,My_softmax.apply,My_Huber_Loss.apply,y,1000,100)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([-0.4053,  1.7032, -0.0622, -0.7004,  1.0270, -0.7382, -1.3955,  1.5659,\n",
            "        -0.1109,  1.5855], requires_grad=True)\n",
            "Iteration: 0 Variable: [0.031 0.253 0.043 0.023 0.129 0.022 0.011 0.221 0.041 0.225] Loss: 0.549616\n",
            "Iteration: 100 Variable: [0.004 0.009 0.947 0.003 0.009 0.003 0.002 0.009 0.005 0.009] Loss: 0.001609\n",
            "Iteration: 200 Variable: [0.003 0.007 0.959 0.002 0.007 0.002 0.001 0.007 0.004 0.007] Loss: 0.000935\n",
            "Iteration: 300 Variable: [0.003 0.006 0.966 0.002 0.006 0.002 0.001 0.006 0.003 0.006] Loss: 0.000664\n",
            "Iteration: 400 Variable: [0.002 0.005 0.97  0.002 0.005 0.002 0.001 0.005 0.003 0.005] Loss: 0.000514\n",
            "Iteration: 500 Variable: [0.002 0.004 0.973 0.002 0.005 0.002 0.001 0.005 0.003 0.004] Loss: 0.000420\n",
            "Iteration: 600 Variable: [0.002 0.004 0.975 0.002 0.004 0.002 0.001 0.004 0.003 0.004] Loss: 0.000354\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: 700 Variable: [0.002 0.004 0.977 0.001 0.004 0.001 0.001 0.004 0.002 0.004] Loss: 0.000307\n",
            "Iteration: 800 Variable: [0.002 0.004 0.978 0.001 0.004 0.001 0.001 0.004 0.002 0.004] Loss: 0.000270\n",
            "Iteration: 900 Variable: [0.002 0.003 0.979 0.001 0.003 0.001 0.001 0.003 0.002 0.003] Loss: 0.000241\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}